{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c443ef8f-e83b-4786-9f1b-bfea10806343",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_objects\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgo\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexpress\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpx\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Carregar o grafo\n",
    "G = nx.read_gml(\"GraphMissingEdges.gml\")\n",
    "\n",
    "# Configurar estilo dos plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üîç AN√ÅLISE ESTRUTURAL DO GRAFO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. AN√ÅLISE DE COMPONENTES CONECTADOS\n",
    "print(\"\\n1. COMPONENTES CONECTADOS:\")\n",
    "num_components = nx.number_connected_components(G)\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "print(f\"N√∫mero de componentes conectados: {num_components}\")\n",
    "print(f\"Tamanho do maior componente: {len(largest_cc)}\")\n",
    "print(f\"Percentual do grafo no maior componente: {len(largest_cc)/len(G.nodes)*100:.1f}%\")\n",
    "\n",
    "# Tamanhos dos componentes\n",
    "component_sizes = [len(c) for c in nx.connected_components(G)]\n",
    "component_sizes.sort(reverse=True)\n",
    "print(f\"Top 5 tamanhos de componentes: {component_sizes[:5]}\")\n",
    "\n",
    "# 2. M√âTRICAS DE CENTRALIDADE\n",
    "print(\"\\n2. AN√ÅLISE DE CENTRALIDADE:\")\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G, k=1000)  # Sample para grafos grandes\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# N√≥s mais centrais\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"Top 5 n√≥s por grau de centralidade:\")\n",
    "for node, cent in top_degree:\n",
    "    print(f\"  Node {node}: {cent:.4f}\")\n",
    "\n",
    "print(\"Top 5 n√≥s por betweenness centrality:\")\n",
    "for node, cent in top_betweenness:\n",
    "    print(f\"  Node {node}: {cent:.4f}\")\n",
    "\n",
    "# 3. PREPARAR DATAFRAME PARA AN√ÅLISES\n",
    "df_nodes = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient=\"index\")\n",
    "df_nodes[\"degree\"] = [G.degree(n) for n in G.nodes()]\n",
    "df_nodes[\"degree_centrality\"] = [degree_centrality[n] for n in G.nodes()]\n",
    "df_nodes[\"betweenness_centrality\"] = [betweenness_centrality[n] for n in G.nodes()]\n",
    "df_nodes[\"closeness_centrality\"] = [closeness_centrality[n] for n in G.nodes()]\n",
    "\n",
    "# Converter colunas num√©ricas\n",
    "df_nodes[\"stars\"] = pd.to_numeric(df_nodes[\"stars\"], errors=\"coerce\")\n",
    "df_nodes[\"reviewCount\"] = pd.to_numeric(df_nodes[\"reviewCount\"], errors=\"coerce\")\n",
    "\n",
    "# Processar categorias\n",
    "df_nodes[\"num_categories\"] = df_nodes[\"categories\"].apply(lambda x: len([c.strip() for c in x.split(\",\") if c.strip()]))\n",
    "df_nodes[\"primary_category\"] = df_nodes[\"categories\"].apply(lambda x: x.split(\",\")[0].strip() if x.strip() else \"Unknown\")\n",
    "\n",
    "# 4. AN√ÅLISE POR CATEGORIAS\n",
    "print(\"\\n3. AN√ÅLISE POR CATEGORIAS:\")\n",
    "category_stats = df_nodes.groupby(\"primary_category\").agg({\n",
    "    'stars': ['count', 'mean', 'std'],\n",
    "    'reviewCount': ['mean', 'std'],\n",
    "    'degree': ['mean', 'std'],\n",
    "    'degree_centrality': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "category_stats.columns = ['_'.join(col).strip() for col in category_stats.columns]\n",
    "category_stats = category_stats.sort_values('stars_count', ascending=False)\n",
    "\n",
    "print(\"Top 10 categorias por n√∫mero de estabelecimentos:\")\n",
    "print(category_stats.head(10)[['stars_count', 'stars_mean', 'reviewCount_mean', 'degree_mean']])\n",
    "\n",
    "# 5. VISUALIZA√á√ïES\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('An√°lises Estruturais do Grafo', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 5.1 Distribui√ß√£o de Graus (log-scale)\n",
    "axes[0,0].hist(df_nodes[\"degree\"], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_xlabel('Grau')\n",
    "axes[0,0].set_ylabel('Frequ√™ncia')\n",
    "axes[0,0].set_title('Distribui√ß√£o de Graus')\n",
    "axes[0,0].set_yscale('log')\n",
    "\n",
    "# 5.2 Distribui√ß√£o de Stars\n",
    "axes[0,1].hist(df_nodes[\"stars\"].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "axes[0,1].set_xlabel('Stars')\n",
    "axes[0,1].set_ylabel('Frequ√™ncia')\n",
    "axes[0,1].set_title('Distribui√ß√£o de Avalia√ß√µes (Stars)')\n",
    "\n",
    "# 5.3 Review Count vs Stars\n",
    "valid_data = df_nodes.dropna(subset=['stars', 'reviewCount'])\n",
    "axes[0,2].scatter(valid_data[\"stars\"], valid_data[\"reviewCount\"], alpha=0.5, s=20)\n",
    "axes[0,2].set_xlabel('Stars')\n",
    "axes[0,2].set_ylabel('Review Count')\n",
    "axes[0,2].set_title('Stars vs Review Count')\n",
    "axes[0,2].set_yscale('log')\n",
    "\n",
    "# 5.4 Degree vs Stars\n",
    "axes[1,0].scatter(valid_data[\"stars\"], valid_data[\"degree\"], alpha=0.5, s=20)\n",
    "axes[1,0].set_xlabel('Stars')\n",
    "axes[1,0].set_ylabel('Degree')\n",
    "axes[1,0].set_title('Avalia√ß√£o vs Conectividade')\n",
    "\n",
    "# 5.5 Centralidade de Grau\n",
    "axes[1,1].hist(df_nodes[\"degree_centrality\"], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1,1].set_xlabel('Degree Centrality')\n",
    "axes[1,1].set_ylabel('Frequ√™ncia')\n",
    "axes[1,1].set_title('Distribui√ß√£o de Centralidade de Grau')\n",
    "\n",
    "# 5.6 N√∫mero de Categorias\n",
    "axes[1,2].hist(df_nodes[\"num_categories\"], bins=range(1, df_nodes[\"num_categories\"].max()+2), \n",
    "               alpha=0.7, edgecolor='black')\n",
    "axes[1,2].set_xlabel('N√∫mero de Categorias')\n",
    "axes[1,2].set_ylabel('Frequ√™ncia')\n",
    "axes[1,2].set_title('Distribui√ß√£o do N√∫mero de Categorias')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. AN√ÅLISE DE CLUSTERS POR CARACTER√çSTICAS\n",
    "print(\"\\n4. AN√ÅLISE DE CLUSTERING:\")\n",
    "\n",
    "# Preparar dados para clustering\n",
    "cluster_features = ['stars', 'reviewCount', 'degree', 'num_categories']\n",
    "cluster_data = df_nodes[cluster_features].dropna()\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(cluster_data_scaled)\n",
    "cluster_data['cluster'] = cluster_labels\n",
    "\n",
    "# An√°lise dos clusters\n",
    "cluster_analysis = cluster_data.groupby('cluster').agg({\n",
    "    'stars': ['count', 'mean'],\n",
    "    'reviewCount': 'mean',\n",
    "    'degree': 'mean',\n",
    "    'num_categories': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Caracter√≠sticas dos clusters:\")\n",
    "print(cluster_analysis)\n",
    "\n",
    "# 7. VISUALIZA√á√ÉO DOS CLUSTERS\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot dos clusters\n",
    "scatter = axes[0].scatter(cluster_data['stars'], cluster_data['reviewCount'], \n",
    "                         c=cluster_data['cluster'], cmap='viridis', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel('Stars')\n",
    "axes[0].set_ylabel('Review Count')\n",
    "axes[0].set_title('Clusters por Stars e Review Count')\n",
    "axes[0].set_yscale('log')\n",
    "plt.colorbar(scatter, ax=axes[0])\n",
    "\n",
    "# Box plot de stars por cluster\n",
    "cluster_data.boxplot(column='stars', by='cluster', ax=axes[1])\n",
    "axes[1].set_title('Distribui√ß√£o de Stars por Cluster')\n",
    "axes[1].set_xlabel('Cluster')\n",
    "\n",
    "plt.suptitle('An√°lise de Clusters', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. AN√ÅLISE DE ASSORTATIVIDADE\n",
    "print(\"\\n5. AN√ÅLISE DE ASSORTATIVIDADE:\")\n",
    "\n",
    "# Assortatividade por grau\n",
    "degree_assortativity = nx.degree_assortativity_coefficient(G)\n",
    "print(f\"Coeficiente de assortatividade por grau: {degree_assortativity:.4f}\")\n",
    "\n",
    "# Assortatividade por atributos categ√≥ricos\n",
    "try:\n",
    "    # Para categorias (usando primary_category)\n",
    "    node_categories = {node: data[\"primary_category\"] for node, data in df_nodes.iterrows()}\n",
    "    nx.set_node_attributes(G, node_categories, \"primary_category\")\n",
    "    category_assortativity = nx.attribute_assortativity_coefficient(G, \"primary_category\")\n",
    "    print(f\"Assortatividade por categoria: {category_assortativity:.4f}\")\n",
    "except:\n",
    "    print(\"N√£o foi poss√≠vel calcular assortatividade por categoria\")\n",
    "\n",
    "# 9. AN√ÅLISE DE SMALL WORLD\n",
    "print(\"\\n6. PROPRIEDADES DE SMALL WORLD:\")\n",
    "if nx.is_connected(G):\n",
    "    avg_path_length = nx.average_shortest_path_length(G)\n",
    "    print(f\"Comprimento m√©dio do caminho: {avg_path_length:.3f}\")\n",
    "else:\n",
    "    # Para o maior componente conectado\n",
    "    largest_cc_subgraph = G.subgraph(largest_cc)\n",
    "    avg_path_length = nx.average_shortest_path_length(largest_cc_subgraph)\n",
    "    print(f\"Comprimento m√©dio do caminho (maior componente): {avg_path_length:.3f}\")\n",
    "\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "print(f\"Coeficiente de clustering m√©dio: {avg_clustering:.4f}\")\n",
    "\n",
    "# Small-world coefficient (aproxima√ß√£o)\n",
    "# Comparar com grafo aleat√≥rio equivalente\n",
    "n = len(G.nodes())\n",
    "m = len(G.edges())\n",
    "avg_degree = 2 * m / n\n",
    "\n",
    "# Clustering esperado em grafo aleat√≥rio\n",
    "random_clustering = avg_degree / (n - 1)\n",
    "# Path length esperado em grafo aleat√≥rio  \n",
    "random_path_length = np.log(n) / np.log(avg_degree)\n",
    "\n",
    "small_world_coeff = (avg_clustering / random_clustering) / (avg_path_length / random_path_length)\n",
    "print(f\"Coeficiente Small World (œÉ): {small_world_coeff:.3f}\")\n",
    "print(\"Valores œÉ > 1 indicam propriedades de small world\")\n",
    "\n",
    "# 10. AN√ÅLISE DE DENSIDADE POR REGI√ÉO/CATEGORIA\n",
    "print(\"\\n7. AN√ÅLISE DE DENSIDADE:\")\n",
    "overall_density = nx.density(G)\n",
    "print(f\"Densidade global do grafo: {overall_density:.6f}\")\n",
    "\n",
    "# Densidade por categoria principal (top 5)\n",
    "top_categories = df_nodes['primary_category'].value_counts().head(5).index\n",
    "\n",
    "for category in top_categories:\n",
    "    cat_nodes = df_nodes[df_nodes['primary_category'] == category].index\n",
    "    subgraph = G.subgraph(cat_nodes)\n",
    "    if len(subgraph.nodes()) > 1:\n",
    "        density = nx.density(subgraph)\n",
    "        print(f\"Densidade para '{category}': {density:.6f}\")\n",
    "\n",
    "# 11. IDENTIFICA√á√ÉO DE HUBS E AUTORIDADES\n",
    "print(\"\\n8. AN√ÅLISE DE HUBS E AUTORIDADES:\")\n",
    "hubs, authorities = nx.hits(G, max_iter=100)\n",
    "\n",
    "# Top hubs e autoridades\n",
    "top_hubs = sorted(hubs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "top_authorities = sorted(authorities.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"Top 5 Hubs:\")\n",
    "for node, score in top_hubs:\n",
    "    node_info = df_nodes.loc[node]\n",
    "    print(f\"  Node {node}: {score:.4f} | Stars: {node_info['stars']:.1f} | Category: {node_info['primary_category']}\")\n",
    "\n",
    "print(\"Top 5 Authorities:\")\n",
    "for node, score in top_authorities:\n",
    "    node_info = df_nodes.loc[node]\n",
    "    print(f\"  Node {node}: {score:.4f} | Stars: {node_info['stars']:.1f} | Category: {node_info['primary_category']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ RESUMO EXECUTIVO:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚Ä¢ Grafo com {len(G.nodes())} n√≥s e {len(G.edges())} arestas\")\n",
    "print(f\"‚Ä¢ {num_components} componentes, maior com {len(largest_cc)/len(G.nodes)*100:.1f}% dos n√≥s\")\n",
    "print(f\"‚Ä¢ Densidade: {overall_density:.6f} (grafo esparso)\")\n",
    "print(f\"‚Ä¢ Clustering m√©dio: {avg_clustering:.4f}\")\n",
    "print(f\"‚Ä¢ Coeficiente Small World: {small_world_coeff:.3f}\")\n",
    "print(f\"‚Ä¢ Assortatividade por grau: {degree_assortativity:.4f}\")\n",
    "print(f\"‚Ä¢ {len(df_nodes['primary_category'].unique())} categorias principais identificadas\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592338d-5c71-45ff-a662-9559df4b90e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
